<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Open Source AI | AI Chronicle Learning Path</title>
  <meta name="description" content="How Llama, DeepSeek, and Mistral proved you don't need billions to build great models. The democratization of frontier AI.">

  <!-- Shared CSS -->
  <link rel="stylesheet" href="../assets/css/design-tokens.css">
  <link rel="stylesheet" href="../assets/css/base.css">
  <link rel="stylesheet" href="../assets/css/nav.css">

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,500;0,8..60,600;1,8..60,400&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">

  <style>
    :root {
      --accent-path: #10b981;
      --accent-glow: rgba(16, 185, 129, 0.2);
      --font-display: 'Playfair Display', Georgia, serif;
      --font-body: 'Source Serif 4', Georgia, serif;
      --font-mono: 'Space Mono', monospace;
    }

    body {
      font-family: var(--font-body);
      font-size: 18px;
      line-height: 1.8;
    }

    ::selection { background: var(--accent-path); color: white; }
    a { color: var(--accent-path); }

    /* Progress Bar */
    .progress-bar {
      position: fixed;
      top: 0;
      left: 0;
      height: 3px;
      background: var(--accent-path);
      z-index: 1001;
      transition: width 50ms linear;
    }

    /* Hero */
    .hero {
      min-height: 80vh;
      display: flex;
      flex-direction: column;
      justify-content: center;
      padding: 6rem 2rem 4rem;
      max-width: 900px;
      margin: 0 auto;
    }

    .hero-breadcrumb {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--color-text-muted);
      margin-bottom: 2rem;
    }
    .hero-breadcrumb a { color: var(--accent-path); }

    .hero-badge {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--accent-path);
      background: var(--accent-glow);
      padding: 0.5rem 1rem;
      border-radius: 100px;
      margin-bottom: 2rem;
      width: fit-content;
    }

    .hero-title {
      font-family: var(--font-display);
      font-size: clamp(2.5rem, 7vw, 4rem);
      font-weight: 400;
      line-height: 1.1;
      letter-spacing: -0.03em;
      margin-bottom: 2rem;
    }

    .hero-subtitle {
      font-size: 1.25rem;
      color: var(--color-text-secondary);
      max-width: 600px;
      margin-bottom: 3rem;
    }

    .hero-meta {
      display: flex;
      gap: 3rem;
      font-family: var(--font-mono);
      font-size: 0.75rem;
      color: var(--color-text-muted);
    }
    .hero-meta-item { display: flex; align-items: center; gap: 0.5rem; }

    /* Article Content */
    .article {
      max-width: 750px;
      margin: 0 auto;
      padding: 0 2rem 4rem;
    }

    .article h2 {
      font-family: var(--font-display);
      font-size: 2rem;
      font-weight: 500;
      margin: 4rem 0 2rem;
      letter-spacing: -0.02em;
    }

    .article h3 {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin: 3rem 0 1rem;
    }

    .article p {
      margin-bottom: 1rem;
      color: var(--color-text-secondary);
    }

    .article strong {
      color: var(--color-text);
      font-weight: 600;
    }

    /* Milestone Card */
    .milestone {
      background: var(--color-bg-elevated);
      border: 1px solid var(--color-border);
      border-left: 4px solid var(--accent-path);
      border-radius: 12px;
      padding: 2rem;
      margin: 3rem 0;
    }

    .milestone-header {
      display: flex;
      justify-content: space-between;
      align-items: flex-start;
      margin-bottom: 1rem;
    }

    .milestone-date {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--accent-path);
    }

    .milestone-significance { font-size: 1.25rem; }

    .milestone-title {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: 0.5rem;
    }

    .milestone-desc {
      color: var(--color-text-secondary);
      margin-bottom: 1rem;
    }

    /* So What Box */
    .so-what {
      background: var(--accent-glow);
      border-radius: 8px;
      padding: 1rem;
      margin-top: 1rem;
    }

    .so-what-label {
      font-family: var(--font-mono);
      font-size: 0.65rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--accent-path);
      margin-bottom: 0.25rem;
    }

    .so-what-text {
      font-size: 0.95rem;
      color: var(--color-text);
      line-height: 1.6;
      margin: 0;
    }

    /* Pull Quote */
    .pull-quote {
      border-left: 3px solid var(--accent-path);
      padding-left: 2rem;
      margin: 3rem 0;
    }

    .pull-quote-text {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-style: italic;
      line-height: 1.4;
      color: var(--color-text);
    }

    .pull-quote-source {
      font-family: var(--font-mono);
      font-size: 0.75rem;
      color: var(--color-text-muted);
      margin-top: 1rem;
    }

    /* Comparison Table */
    .comparison-table {
      width: 100%;
      border-collapse: collapse;
      margin: 3rem 0;
      font-size: 0.9rem;
    }

    .comparison-table th,
    .comparison-table td {
      padding: 1rem;
      text-align: left;
      border-bottom: 1px solid var(--color-border);
    }

    .comparison-table th {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--color-text-muted);
      font-weight: 400;
    }

    .comparison-table td {
      color: var(--color-text-secondary);
    }

    .comparison-table td:first-child {
      font-weight: 600;
      color: var(--color-text);
    }

    /* Key Takeaways */
    .takeaways {
      background: var(--color-bg-elevated);
      border: 1px solid var(--color-border);
      border-radius: 16px;
      padding: 3rem;
      margin: 4rem 0;
    }

    .takeaways-title {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: 2rem;
    }

    .takeaways-list { list-style: none; }

    .takeaways-list li {
      position: relative;
      padding-left: 2rem;
      margin-bottom: 1rem;
      color: var(--color-text-secondary);
    }

    .takeaways-list li::before {
      content: '\2192';
      position: absolute;
      left: 0;
      color: var(--accent-path);
    }

    /* Timeline */
    .timeline {
      position: relative;
      margin: 4rem 0;
      padding-left: 3rem;
    }

    .timeline::before {
      content: '';
      position: absolute;
      left: 8px;
      top: 0;
      bottom: 0;
      width: 2px;
      background: var(--color-border);
    }

    .timeline-item {
      position: relative;
      margin-bottom: 2rem;
      padding-bottom: 2rem;
      border-bottom: 1px solid var(--color-border);
    }

    .timeline-item:last-child {
      border-bottom: none;
      margin-bottom: 0;
      padding-bottom: 0;
    }

    .timeline-item::before {
      content: '';
      position: absolute;
      left: calc(-3rem + 4px);
      top: 6px;
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: var(--accent-path);
    }

    .timeline-date {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--accent-path);
      margin-bottom: 0.25rem;
    }

    .timeline-title {
      font-family: var(--font-display);
      font-size: 1.1rem;
      font-weight: 500;
      margin-bottom: 0.25rem;
    }

    .timeline-desc {
      font-size: 0.9rem;
      color: var(--color-text-muted);
    }

    /* Next Path CTA */
    .next-path {
      background: var(--color-bg-elevated);
      border: 1px solid var(--color-border);
      border-radius: 16px;
      padding: 3rem;
      margin: 4rem 0;
      text-align: center;
    }

    .next-path-label {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--color-text-muted);
      margin-bottom: 1rem;
    }

    .next-path-title {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: 1rem;
    }

    .next-path-link {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      font-family: var(--font-mono);
      font-size: 0.85rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--accent-path);
      padding: 1rem 2rem;
      border: 1px solid var(--accent-path);
      border-radius: 100px;
      transition: all 0.15s ease;
    }

    .next-path-link:hover {
      background: var(--accent-path);
      color: white;
      text-decoration: none;
    }

    /* Footer */
    .footer {
      padding: 3rem;
      border-top: 1px solid var(--color-border);
      text-align: center;
    }

    .footer-text {
      font-size: 0.85rem;
      color: var(--color-text-muted);
    }
    .footer-text a { color: var(--color-text-secondary); }

    /* Responsive */
    @media (max-width: 768px) {
      .hero-meta { flex-direction: column; gap: 0.5rem; }
      .milestone-header { flex-direction: column; gap: 0.5rem; }
      .comparison-table { font-size: 0.8rem; }
      .comparison-table th,
      .comparison-table td { padding: 0.5rem; }
    }
  </style>
</head>
<body>
  <!-- Progress Bar -->
  <div class="progress-bar" id="progressBar"></div>

  <!-- Global Navigation -->
  <nav class="global-nav" id="globalNav">
    <div class="nav-container">
      <a href="../index.html" class="nav-logo">AI Chronicle</a>
      <div class="nav-links">
        <a href="../timeline.html" class="nav-link">Timeline</a>
        <a href="index.html" class="nav-link">Learn</a>
        <a href="../docs.html#/digests/2025/README" class="nav-link">Digests</a>
      </div>
      <div class="nav-actions">
        <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
          <svg class="icon-sun" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <circle cx="12" cy="12" r="5"/>
            <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
          </svg>
          <svg class="icon-moon" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/>
          </svg>
        </button>
        <a href="https://github.com/anthropics/awesome-ai-chronicle" class="nav-github" aria-label="GitHub">
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M12 0C5.37 0 0 5.37 0 12c0 5.31 3.435 9.795 8.205 11.385.6.105.825-.255.825-.57 0-.285-.015-1.23-.015-2.235-3.015.555-3.795-.735-4.035-1.41-.135-.345-.72-1.41-1.23-1.695-.42-.225-1.02-.78-.015-.795.945-.015 1.62.87 1.845 1.23 1.08 1.815 2.805 1.305 3.495.99.105-.78.42-1.305.765-1.605-2.67-.3-5.46-1.335-5.46-5.925 0-1.305.465-2.385 1.23-3.225-.12-.3-.54-1.53.12-3.18 0 0 1.005-.315 3.3 1.23.96-.27 1.98-.405 3-.405s2.04.135 3 .405c2.295-1.56 3.3-1.23 3.3-1.23.66 1.65.24 2.88.12 3.18.765.84 1.23 1.905 1.23 3.225 0 4.605-2.805 5.625-5.475 5.925.435.375.81 1.095.81 2.22 0 1.605-.015 2.895-.015 3.3 0 .315.225.69.825.57A12.02 12.02 0 0024 12c0-6.63-5.37-12-12-12z"/>
          </svg>
        </a>
      </div>
    </div>
  </nav>

  <!-- Hero -->
  <header class="hero">
    <div class="hero-breadcrumb">
      <a href="index.html">Learning Paths</a> / Open Source AI
    </div>
    <div class="hero-badge">
      <span>&#127760;</span>
      Learning Path
    </div>
    <h1 class="hero-title">Open Source AI</h1>
    <p class="hero-subtitle">
      How Llama, DeepSeek, and Mistral proved you don't need billions to build great models.
      The movement that democratized frontier AI.
    </p>
    <div class="hero-meta">
      <div class="hero-meta-item">
        <span>&#128214;</span>
        <span>~25 min read</span>
      </div>
      <div class="hero-meta-item">
        <span>&#128205;</span>
        <span>15 key events</span>
      </div>
      <div class="hero-meta-item">
        <span>&#128197;</span>
        <span>Jul 2023 - Dec 2025</span>
      </div>
    </div>
  </header>

  <!-- Article Content -->
  <article class="article">
    <h2>The Closed Model Era</h2>

    <p>
      In early 2023, frontier AI was a walled garden. GPT-4 sat behind OpenAI's API.
      Claude required Anthropic access. Google's best models were internal only.
      The message was clear: <strong>building great AI required billions of dollars
      and thousands of GPUs</strong>.
    </p>

    <p>
      Researchers could study papers, but not weights. Startups could use APIs, but
      not customize models. The AI revolution was happening, but most of the world
      could only watch through a paywall.
    </p>

    <blockquote class="pull-quote">
      <p class="pull-quote-text">
        "Open source is the path to widespread AI benefits. Closed development concentrates
        power in the hands of a few."
      </p>
      <p class="pull-quote-source">- Mark Zuckerberg, July 2023</p>
    </blockquote>

    <h2>The Llama That Changed Everything</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">July 18, 2023</span>
        <span class="milestone-significance">&#128293;</span>
      </div>
      <h3 class="milestone-title">Llama 2 Open-Sourced</h3>
      <p class="milestone-desc">
        Meta released Llama 2 (7B, 13B, 70B parameters) under a permissive license allowing
        both research and commercial use. For the first time, a model competitive with
        GPT-3.5 was freely available to anyone.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> Llama 2 meant you could run a capable LLM
          on your own infrastructure. No API costs, no data leaving your servers, full
          control over fine-tuning. The era of "build vs. buy" began.
        </p>
      </div>
    </div>

    <p>
      Llama 2 sparked an explosion of innovation. Within months, the Hugging Face
      model hub was flooded with fine-tuned variants. <strong>Mistral</strong>, a Paris-based
      startup founded by ex-Google and ex-Meta researchers, released Mistral 7B-a
      model that punched far above its weight class.
    </p>

    <p>
      The open-source community discovered that smaller, well-trained models could
      compete with giants. Quantization techniques let 70B models run on consumer
      GPUs. The "local LLM" movement was born.
    </p>

    <h2>Llama 3: Closing the Gap</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">April 18, 2024</span>
        <span class="milestone-significance">&#11088;</span>
      </div>
      <h3 class="milestone-title">Llama 3 Released</h3>
      <p class="milestone-desc">
        Meta released Llama 3 (8B and 70B) with improved performance and an 8K context
        window. The gap between open and closed models narrowed significantly-Llama 3 70B
        approached GPT-4 on many benchmarks.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> Llama 3 was "good enough" for most production
          use cases. Companies could now build products on open weights without sacrificing
          quality. The business case for closed APIs weakened.
        </p>
      </div>
    </div>

    <h2>The Sputnik Moment: DeepSeek R1</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">January 20, 2025</span>
        <span class="milestone-significance">&#128293;</span>
      </div>
      <h3 class="milestone-title">DeepSeek R1: Open-Source Reasoning</h3>
      <p class="milestone-desc">
        Chinese lab DeepSeek released R1, an open-source reasoning model matching OpenAI's o1.
        Training cost: approximately <strong>$6 million</strong>-a fraction of the assumed
        $100M+ required for frontier models. The weights were immediately available on
        Hugging Face.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> R1 proved that reasoning capabilities-previously
          thought to require massive compute-could be achieved efficiently. You could now
          run a model that "thinks" on your own hardware. The monopoly on reasoning was broken.
        </p>
      </div>
    </div>

    <p>
      The market reaction was immediate and brutal. Nvidia lost <strong>$500 billion</strong>
      in market cap in a single day as investors questioned whether expensive AI infrastructure
      was truly necessary. The "Sputnik moment" comparison spread through tech media.
    </p>

    <p>
      DeepSeek's success came from efficiency innovations: better training data curation,
      clever architecture choices, and a focus on reasoning-specific optimization. They
      proved that brute-force scaling wasn't the only path to frontier performance.
    </p>

    <h2>Llama 4: The MoE Revolution</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">April 5, 2025</span>
        <span class="milestone-significance">&#128293;</span>
      </div>
      <h3 class="milestone-title">Llama 4 Family Release</h3>
      <p class="milestone-desc">
        Meta released Llama 4 as the first open-weight model family with <strong>Mixture-of-Experts
        (MoE)</strong> architecture and native multimodality. Scout, Maverick, and the planned
        Behemoth variants offered unprecedented capability at various scales.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> MoE architecture means you only activate a
          fraction of parameters per token-massive capability with manageable compute.
          Llama 4 Scout runs on a single H100 while offering frontier performance.
        </p>
      </div>
    </div>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">April 5, 2025</span>
        <span class="milestone-significance">&#11088;</span>
      </div>
      <h3 class="milestone-title">Llama 4 Scout: 10M Token Context</h3>
      <p class="milestone-desc">
        Llama 4 Scout offered an unprecedented <strong>10 million token context window</strong>
        while fitting on a single Nvidia H100 GPU with quantization. This enabled entirely
        new use cases: analyzing entire codebases, processing book-length documents, maintaining
        extensive conversation histories.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> 10M context means you can fit an entire
          codebase, documentation set, or research corpus in a single prompt. RAG becomes
          optional for many use cases. The "context window problem" is solved.
        </p>
      </div>
    </div>

    <h2>The Open-Closed Gap: 2025</h2>

    <table class="comparison-table">
      <thead>
        <tr>
          <th>Capability</th>
          <th>Best Open Model</th>
          <th>Best Closed Model</th>
          <th>Gap</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>General Reasoning</td>
          <td>DeepSeek R1</td>
          <td>o1</td>
          <td>~Equal</td>
        </tr>
        <tr>
          <td>Code Generation</td>
          <td>Llama 4 Maverick</td>
          <td>Claude Opus 4.5</td>
          <td>~5%</td>
        </tr>
        <tr>
          <td>Context Length</td>
          <td>Llama 4 Scout (10M)</td>
          <td>Gemini 1.5 (2M)</td>
          <td>Open leads</td>
        </tr>
        <tr>
          <td>Multilingual</td>
          <td>DeepSeek V3</td>
          <td>GPT-5.1</td>
          <td>~Equal</td>
        </tr>
        <tr>
          <td>Multimodal</td>
          <td>Llama 4</td>
          <td>Gemini 3</td>
          <td>~10%</td>
        </tr>
      </tbody>
    </table>

    <p>
      By late 2025, the gap between open and closed models had effectively closed on most
      benchmarks. In some areas-particularly context length-open models actually led.
      The narrative shifted from "can open source compete?" to "why pay for closed APIs?"
    </p>

    <h2>The Global Open Source Ecosystem</h2>

    <p>
      Open-source AI became a global movement. Beyond Meta and DeepSeek, key players emerged:
    </p>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">Throughout 2025</span>
        <span class="milestone-significance">&#11088;</span>
      </div>
      <h3 class="milestone-title">Global Open Source Labs</h3>
      <p class="milestone-desc">
        <strong>Mistral AI (France)</strong> continued releasing efficient models that punched above
        their weight. <strong>Alibaba's Qwen</strong> models dominated Chinese-language tasks.
        <strong>G42's Falcon</strong> (UAE) targeted Arabic markets. <strong>AI2's OLMo</strong>
        provided fully transparent training for researchers.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> Open source is no longer a single company's effort.
          Multiple competitive options exist for different use cases, languages, and regions.
          The ecosystem is robust and self-sustaining.
        </p>
      </div>
    </div>

    <h2>The Full Timeline</h2>

    <div class="timeline">
      <div class="timeline-item">
        <div class="timeline-date">February 2023</div>
        <div class="timeline-title">Llama 1 Leaked</div>
        <div class="timeline-desc">Meta's original Llama leaked online, sparking open-source interest</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">July 2023</div>
        <div class="timeline-title">Llama 2 Released</div>
        <div class="timeline-desc">First officially open commercial-use LLM at scale</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">September 2023</div>
        <div class="timeline-title">Mistral 7B</div>
        <div class="timeline-desc">French startup releases surprisingly capable small model</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">December 2023</div>
        <div class="timeline-title">Mixtral 8x7B</div>
        <div class="timeline-desc">First open MoE model demonstrates efficiency gains</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">April 2024</div>
        <div class="timeline-title">Llama 3</div>
        <div class="timeline-desc">Meta closes gap with GPT-4 class performance</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">November 2024</div>
        <div class="timeline-title">DeepSeek V2</div>
        <div class="timeline-desc">Chinese lab demonstrates efficient training methods</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">January 2025</div>
        <div class="timeline-title">DeepSeek R1</div>
        <div class="timeline-desc">Open-source reasoning matches o1, $500B Nvidia drop</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">April 2025</div>
        <div class="timeline-title">Llama 4 Family</div>
        <div class="timeline-desc">First open MoE + multimodal + 10M context model</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">April 2025</div>
        <div class="timeline-title">G42 Falcon 3</div>
        <div class="timeline-desc">UAE releases Arabic-optimized open models</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">July 2025</div>
        <div class="timeline-title">AI2 OLMo 2</div>
        <div class="timeline-desc">Fully transparent training for research community</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">October 2025</div>
        <div class="timeline-title">Llama Federal Approval</div>
        <div class="timeline-desc">US government approves Llama for federal use</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">November 2025</div>
        <div class="timeline-title">DeepSeek V3 Preview</div>
        <div class="timeline-desc">China continues pushing open-source frontier</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">December 2025</div>
        <div class="timeline-title">DeepSeek IMO Gold</div>
        <div class="timeline-desc">Open-source model achieves math olympiad gold</div>
      </div>
    </div>

    <h2>Key Takeaways for Practitioners</h2>

    <div class="takeaways">
      <h3 class="takeaways-title">What This Means For You</h3>
      <ul class="takeaways-list">
        <li>
          <strong>Open source is production-ready.</strong> Llama 4 and DeepSeek R1 are
          competitive with the best closed models. You can build products without API
          dependencies.
        </li>
        <li>
          <strong>Self-hosting is viable.</strong> Quantization and MoE architectures mean
          you can run capable models on reasonable hardware. The economics favor ownership
          for sustained workloads.
        </li>
        <li>
          <strong>Fine-tuning is your moat.</strong> Open weights let you customize models
          for your domain. This creates differentiation that API access can't provide.
        </li>
        <li>
          <strong>The ecosystem is mature.</strong> Hugging Face, vLLM, and countless tools
          make open-source deployment straightforward. You're not pioneering anymore.
        </li>
        <li>
          <strong>Watch DeepSeek and Qwen.</strong> Chinese labs are shipping frontier-class
          open models with innovative efficiency techniques. Don't ignore them because of
          geography.
        </li>
      </ul>
    </div>

    <!-- Next Path -->
    <div class="next-path">
      <div class="next-path-label">Continue Learning</div>
      <h3 class="next-path-title">Rise of AI Agents</h3>
      <p style="color: var(--color-text-secondary); margin-bottom: 2rem;">
        From chatbots to autonomous workers. How Claude Computer Use, ChatGPT Agent,
        and MCP are enabling AI to take action.
      </p>
      <a href="ai-agents.html" class="next-path-link">
        Start Next Path
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <path d="M5 12h14M12 5l7 7-7 7"/>
        </svg>
      </a>
    </div>
  </article>

  <!-- Footer -->
  <footer class="footer">
    <p class="footer-text">
      <a href="index.html">All Learning Paths</a> ·
      <a href="../timeline.html">Visual Timeline</a> ·
      <a href="../docs.html">Documentation</a>
    </p>
  </footer>

  <!-- Shared JS -->
  <script src="../assets/js/theme-toggle.js"></script>
  <script src="../assets/js/nav.js"></script>

  <script>
    // Progress bar
    const progressBar = document.getElementById('progressBar');
    window.addEventListener('scroll', () => {
      const scrollTop = window.scrollY;
      const docHeight = document.documentElement.scrollHeight - window.innerHeight;
      const progress = (scrollTop / docHeight) * 100;
      progressBar.style.width = progress + '%';
    });
  </script>
</body>
</html>
