<!DOCTYPE html>
<html lang="en" data-theme="dark">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Open Source AI | AI Chronicle Learning Path</title>
  <meta name="description" content="How Llama, DeepSeek, and Mistral proved you don't need billions to build great models. The democratization of frontier AI.">

  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,500;0,8..60,600;1,8..60,400&family=Space+Mono:wght@400;700&display=swap" rel="stylesheet">

  <style>
    :root {
      --font-display: 'Playfair Display', Georgia, serif;
      --font-body: 'Source Serif 4', Georgia, serif;
      --font-mono: 'Space Mono', monospace;
      --space-xs: 0.25rem;
      --space-sm: 0.5rem;
      --space-md: 1rem;
      --space-lg: 2rem;
      --space-xl: 4rem;
      --space-2xl: 6rem;
      --transition-fast: 150ms ease;
      --transition-base: 300ms ease;
      --accent-path: #10b981;
    }

    [data-theme="dark"] {
      --bg-primary: #0a0a0a;
      --bg-secondary: #141414;
      --bg-tertiary: #1a1a1a;
      --text-primary: #fafafa;
      --text-secondary: #a3a3a3;
      --text-tertiary: #737373;
      --text-muted: #525252;
      --accent-primary: #3b82f6;
      --accent-glow: rgba(16, 185, 129, 0.2);
      --border-subtle: rgba(255, 255, 255, 0.06);
      --border-default: rgba(255, 255, 255, 0.1);
      --shadow-lg: 0 8px 32px rgba(0, 0, 0, 0.5);
    }

    [data-theme="light"] {
      --bg-primary: #fafafa;
      --bg-secondary: #f5f5f5;
      --bg-tertiary: #ebebeb;
      --text-primary: #0a0a0a;
      --text-secondary: #525252;
      --text-tertiary: #737373;
      --text-muted: #a3a3a3;
      --accent-primary: #2563eb;
      --accent-glow: rgba(5, 150, 105, 0.15);
      --border-subtle: rgba(0, 0, 0, 0.04);
      --border-default: rgba(0, 0, 0, 0.08);
      --shadow-lg: 0 8px 32px rgba(0, 0, 0, 0.12);
    }

    *, *::before, *::after { box-sizing: border-box; margin: 0; padding: 0; }
    html { scroll-behavior: smooth; }
    body {
      font-family: var(--font-body);
      font-size: 18px;
      line-height: 1.8;
      color: var(--text-primary);
      background: var(--bg-primary);
      -webkit-font-smoothing: antialiased;
    }
    ::selection { background: var(--accent-path); color: white; }
    a { color: var(--accent-path); text-decoration: none; }
    a:hover { text-decoration: underline; }

    .progress-bar {
      position: fixed;
      top: 0;
      left: 0;
      height: 3px;
      background: var(--accent-path);
      z-index: 1001;
      transition: width 50ms linear;
    }

    .nav {
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      z-index: 1000;
      padding: var(--space-md) var(--space-lg);
      display: flex;
      justify-content: space-between;
      align-items: center;
      background: transparent;
      transition: background var(--transition-base);
    }
    .nav.scrolled {
      background: rgba(10, 10, 10, 0.9);
      backdrop-filter: blur(20px);
    }
    [data-theme="light"] .nav.scrolled {
      background: rgba(250, 250, 250, 0.9);
    }
    .nav-logo {
      font-family: var(--font-display);
      font-size: 1.25rem;
      font-weight: 500;
      color: var(--text-primary);
    }
    .nav-links { display: flex; gap: var(--space-lg); align-items: center; }
    .nav-link {
      font-family: var(--font-mono);
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--text-secondary);
    }
    .theme-toggle {
      background: transparent;
      border: 1px solid var(--border-default);
      color: var(--text-secondary);
      width: 40px;
      height: 40px;
      border-radius: 50%;
      cursor: pointer;
      display: flex;
      align-items: center;
      justify-content: center;
    }
    .theme-toggle svg { width: 18px; height: 18px; }

    .hero {
      min-height: 80vh;
      display: flex;
      flex-direction: column;
      justify-content: center;
      padding: var(--space-2xl) var(--space-lg);
      max-width: 900px;
      margin: 0 auto;
    }
    .hero-breadcrumb {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--text-tertiary);
      margin-bottom: var(--space-lg);
    }
    .hero-breadcrumb a { color: var(--accent-path); }
    .hero-badge {
      display: inline-flex;
      align-items: center;
      gap: var(--space-sm);
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--accent-path);
      background: var(--accent-glow);
      padding: var(--space-sm) var(--space-md);
      border-radius: 100px;
      margin-bottom: var(--space-lg);
      width: fit-content;
    }
    .hero-title {
      font-family: var(--font-display);
      font-size: clamp(2.5rem, 7vw, 4rem);
      font-weight: 400;
      line-height: 1.1;
      letter-spacing: -0.03em;
      margin-bottom: var(--space-lg);
    }
    .hero-subtitle {
      font-size: 1.25rem;
      color: var(--text-secondary);
      max-width: 600px;
      margin-bottom: var(--space-xl);
    }
    .hero-meta {
      display: flex;
      gap: var(--space-xl);
      font-family: var(--font-mono);
      font-size: 0.75rem;
      color: var(--text-tertiary);
    }
    .hero-meta-item { display: flex; align-items: center; gap: var(--space-sm); }

    .article {
      max-width: 750px;
      margin: 0 auto;
      padding: 0 var(--space-lg) var(--space-2xl);
    }
    .article h2 {
      font-family: var(--font-display);
      font-size: 2rem;
      font-weight: 500;
      margin: var(--space-2xl) 0 var(--space-lg);
      letter-spacing: -0.02em;
    }
    .article h3 {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin: var(--space-xl) 0 var(--space-md);
    }
    .article p {
      margin-bottom: var(--space-md);
      color: var(--text-secondary);
    }
    .article strong {
      color: var(--text-primary);
      font-weight: 600;
    }

    .milestone {
      background: var(--bg-secondary);
      border: 1px solid var(--border-subtle);
      border-left: 4px solid var(--accent-path);
      border-radius: 12px;
      padding: var(--space-lg);
      margin: var(--space-xl) 0;
    }
    .milestone-header {
      display: flex;
      justify-content: space-between;
      align-items: flex-start;
      margin-bottom: var(--space-md);
    }
    .milestone-date {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--accent-path);
    }
    .milestone-significance { font-size: 1.25rem; }
    .milestone-title {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: var(--space-sm);
    }
    .milestone-desc {
      color: var(--text-secondary);
      margin-bottom: var(--space-md);
    }

    .so-what {
      background: var(--accent-glow);
      border-radius: 8px;
      padding: var(--space-md);
      margin-top: var(--space-md);
    }
    .so-what-label {
      font-family: var(--font-mono);
      font-size: 0.65rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--accent-path);
      margin-bottom: var(--space-xs);
    }
    .so-what-text {
      font-size: 0.95rem;
      color: var(--text-primary);
      line-height: 1.6;
      margin: 0;
    }

    .pull-quote {
      border-left: 3px solid var(--accent-path);
      padding-left: var(--space-lg);
      margin: var(--space-xl) 0;
    }
    .pull-quote-text {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-style: italic;
      line-height: 1.4;
      color: var(--text-primary);
    }
    .pull-quote-source {
      font-family: var(--font-mono);
      font-size: 0.75rem;
      color: var(--text-tertiary);
      margin-top: var(--space-md);
    }

    .takeaways {
      background: var(--bg-secondary);
      border: 1px solid var(--border-subtle);
      border-radius: 16px;
      padding: var(--space-xl);
      margin: var(--space-2xl) 0;
    }
    .takeaways-title {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: var(--space-lg);
    }
    .takeaways-list { list-style: none; }
    .takeaways-list li {
      position: relative;
      padding-left: var(--space-lg);
      margin-bottom: var(--space-md);
      color: var(--text-secondary);
    }
    .takeaways-list li::before {
      content: '‚Üí';
      position: absolute;
      left: 0;
      color: var(--accent-path);
    }

    .timeline {
      position: relative;
      margin: var(--space-2xl) 0;
      padding-left: var(--space-xl);
    }
    .timeline::before {
      content: '';
      position: absolute;
      left: 8px;
      top: 0;
      bottom: 0;
      width: 2px;
      background: var(--border-default);
    }
    .timeline-item {
      position: relative;
      margin-bottom: var(--space-lg);
      padding-bottom: var(--space-lg);
      border-bottom: 1px solid var(--border-subtle);
    }
    .timeline-item:last-child {
      border-bottom: none;
      margin-bottom: 0;
      padding-bottom: 0;
    }
    .timeline-item::before {
      content: '';
      position: absolute;
      left: calc(-1 * var(--space-xl) + 4px);
      top: 6px;
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: var(--accent-path);
    }
    .timeline-date {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--accent-path);
      margin-bottom: var(--space-xs);
    }
    .timeline-title {
      font-family: var(--font-display);
      font-size: 1.1rem;
      font-weight: 500;
      margin-bottom: var(--space-xs);
    }
    .timeline-desc {
      font-size: 0.9rem;
      color: var(--text-tertiary);
    }

    .comparison-table {
      width: 100%;
      border-collapse: collapse;
      margin: var(--space-xl) 0;
      font-size: 0.9rem;
    }
    .comparison-table th,
    .comparison-table td {
      padding: var(--space-md);
      text-align: left;
      border-bottom: 1px solid var(--border-subtle);
    }
    .comparison-table th {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--text-tertiary);
      font-weight: 400;
    }
    .comparison-table td {
      color: var(--text-secondary);
    }
    .comparison-table td:first-child {
      font-weight: 600;
      color: var(--text-primary);
    }

    .next-path {
      background: var(--bg-secondary);
      border: 1px solid var(--border-subtle);
      border-radius: 16px;
      padding: var(--space-xl);
      margin: var(--space-2xl) 0;
      text-align: center;
    }
    .next-path-label {
      font-family: var(--font-mono);
      font-size: 0.7rem;
      text-transform: uppercase;
      letter-spacing: 0.15em;
      color: var(--text-tertiary);
      margin-bottom: var(--space-md);
    }
    .next-path-title {
      font-family: var(--font-display);
      font-size: 1.5rem;
      font-weight: 500;
      margin-bottom: var(--space-md);
    }
    .next-path-link {
      display: inline-flex;
      align-items: center;
      gap: var(--space-sm);
      font-family: var(--font-mono);
      font-size: 0.85rem;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--accent-path);
      padding: var(--space-md) var(--space-lg);
      border: 1px solid var(--accent-path);
      border-radius: 100px;
      transition: all var(--transition-fast);
    }
    .next-path-link:hover {
      background: var(--accent-path);
      color: white;
      text-decoration: none;
    }

    .footer {
      padding: var(--space-xl);
      border-top: 1px solid var(--border-subtle);
      text-align: center;
    }
    .footer-text {
      font-size: 0.85rem;
      color: var(--text-muted);
    }
    .footer-text a { color: var(--text-tertiary); }

    @media (max-width: 768px) {
      .nav-links { display: none; }
      .hero-meta { flex-direction: column; gap: var(--space-sm); }
      .milestone-header { flex-direction: column; gap: var(--space-sm); }
      .comparison-table { font-size: 0.8rem; }
      .comparison-table th,
      .comparison-table td { padding: var(--space-sm); }
    }
  </style>
</head>
<body>
  <div class="progress-bar" id="progressBar"></div>

  <nav class="nav" id="nav">
    <a href="../index.html" class="nav-logo">AI Chronicle</a>
    <div class="nav-links">
      <a href="index.html" class="nav-link">All Paths</a>
      <a href="../timeline.html" class="nav-link">Timeline</a>
      <button class="theme-toggle" id="themeToggle" aria-label="Toggle theme">
        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <circle cx="12" cy="12" r="5"/>
          <path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>
        </svg>
      </button>
    </div>
  </nav>

  <header class="hero">
    <div class="hero-breadcrumb">
      <a href="index.html">Learning Paths</a> / Open Source AI
    </div>
    <div class="hero-badge">
      <span>üåê</span>
      Learning Path
    </div>
    <h1 class="hero-title">Open Source AI</h1>
    <p class="hero-subtitle">
      How Llama, DeepSeek, and Mistral proved you don't need billions to build great models.
      The movement that democratized frontier AI.
    </p>
    <div class="hero-meta">
      <div class="hero-meta-item">
        <span>üìñ</span>
        <span>~25 min read</span>
      </div>
      <div class="hero-meta-item">
        <span>üìç</span>
        <span>15 key events</span>
      </div>
      <div class="hero-meta-item">
        <span>üìÖ</span>
        <span>Jul 2023 ‚Äî Dec 2025</span>
      </div>
    </div>
  </header>

  <article class="article">
    <h2>The Closed Model Era</h2>

    <p>
      In early 2023, frontier AI was a walled garden. GPT-4 sat behind OpenAI's API.
      Claude required Anthropic access. Google's best models were internal only.
      The message was clear: <strong>building great AI required billions of dollars
      and thousands of GPUs</strong>.
    </p>

    <p>
      Researchers could study papers, but not weights. Startups could use APIs, but
      not customize models. The AI revolution was happening, but most of the world
      could only watch through a paywall.
    </p>

    <blockquote class="pull-quote">
      <p class="pull-quote-text">
        "Open source is the path to widespread AI benefits. Closed development concentrates
        power in the hands of a few."
      </p>
      <p class="pull-quote-source">‚Äî Mark Zuckerberg, July 2023</p>
    </blockquote>

    <h2>The Llama That Changed Everything</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">July 18, 2023</span>
        <span class="milestone-significance">üî•</span>
      </div>
      <h3 class="milestone-title">Llama 2 Open-Sourced</h3>
      <p class="milestone-desc">
        Meta released Llama 2 (7B, 13B, 70B parameters) under a permissive license allowing
        both research and commercial use. For the first time, a model competitive with
        GPT-3.5 was freely available to anyone.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> Llama 2 meant you could run a capable LLM
          on your own infrastructure. No API costs, no data leaving your servers, full
          control over fine-tuning. The era of "build vs. buy" began.
        </p>
      </div>
    </div>

    <p>
      Llama 2 sparked an explosion of innovation. Within months, the Hugging Face
      model hub was flooded with fine-tuned variants. <strong>Mistral</strong>, a Paris-based
      startup founded by ex-Google and ex-Meta researchers, released Mistral 7B‚Äîa
      model that punched far above its weight class.
    </p>

    <p>
      The open-source community discovered that smaller, well-trained models could
      compete with giants. Quantization techniques let 70B models run on consumer
      GPUs. The "local LLM" movement was born.
    </p>

    <h2>Llama 3: Closing the Gap</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">April 18, 2024</span>
        <span class="milestone-significance">‚≠ê</span>
      </div>
      <h3 class="milestone-title">Llama 3 Released</h3>
      <p class="milestone-desc">
        Meta released Llama 3 (8B and 70B) with improved performance and an 8K context
        window. The gap between open and closed models narrowed significantly‚ÄîLlama 3 70B
        approached GPT-4 on many benchmarks.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> Llama 3 was "good enough" for most production
          use cases. Companies could now build products on open weights without sacrificing
          quality. The business case for closed APIs weakened.
        </p>
      </div>
    </div>

    <h2>The Sputnik Moment: DeepSeek R1</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">January 20, 2025</span>
        <span class="milestone-significance">üî•</span>
      </div>
      <h3 class="milestone-title">DeepSeek R1: Open-Source Reasoning</h3>
      <p class="milestone-desc">
        Chinese lab DeepSeek released R1, an open-source reasoning model matching OpenAI's o1.
        Training cost: approximately <strong>$6 million</strong>‚Äîa fraction of the assumed
        $100M+ required for frontier models. The weights were immediately available on
        Hugging Face.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> R1 proved that reasoning capabilities‚Äîpreviously
          thought to require massive compute‚Äîcould be achieved efficiently. You could now
          run a model that "thinks" on your own hardware. The monopoly on reasoning was broken.
        </p>
      </div>
    </div>

    <p>
      The market reaction was immediate and brutal. Nvidia lost <strong>$500 billion</strong>
      in market cap in a single day as investors questioned whether expensive AI infrastructure
      was truly necessary. The "Sputnik moment" comparison spread through tech media.
    </p>

    <p>
      DeepSeek's success came from efficiency innovations: better training data curation,
      clever architecture choices, and a focus on reasoning-specific optimization. They
      proved that brute-force scaling wasn't the only path to frontier performance.
    </p>

    <h2>Llama 4: The MoE Revolution</h2>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">April 5, 2025</span>
        <span class="milestone-significance">üî•</span>
      </div>
      <h3 class="milestone-title">Llama 4 Family Release</h3>
      <p class="milestone-desc">
        Meta released Llama 4 as the first open-weight model family with <strong>Mixture-of-Experts
        (MoE)</strong> architecture and native multimodality. Scout, Maverick, and the planned
        Behemoth variants offered unprecedented capability at various scales.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> MoE architecture means you only activate a
          fraction of parameters per token‚Äîmassive capability with manageable compute.
          Llama 4 Scout runs on a single H100 while offering frontier performance.
        </p>
      </div>
    </div>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">April 5, 2025</span>
        <span class="milestone-significance">‚≠ê</span>
      </div>
      <h3 class="milestone-title">Llama 4 Scout: 10M Token Context</h3>
      <p class="milestone-desc">
        Llama 4 Scout offered an unprecedented <strong>10 million token context window</strong>
        while fitting on a single Nvidia H100 GPU with quantization. This enabled entirely
        new use cases: analyzing entire codebases, processing book-length documents, maintaining
        extensive conversation histories.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> 10M context means you can fit an entire
          codebase, documentation set, or research corpus in a single prompt. RAG becomes
          optional for many use cases. The "context window problem" is solved.
        </p>
      </div>
    </div>

    <h2>The Open-Closed Gap: 2025</h2>

    <table class="comparison-table">
      <thead>
        <tr>
          <th>Capability</th>
          <th>Best Open Model</th>
          <th>Best Closed Model</th>
          <th>Gap</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>General Reasoning</td>
          <td>DeepSeek R1</td>
          <td>o1</td>
          <td>~Equal</td>
        </tr>
        <tr>
          <td>Code Generation</td>
          <td>Llama 4 Maverick</td>
          <td>Claude Opus 4.5</td>
          <td>~5%</td>
        </tr>
        <tr>
          <td>Context Length</td>
          <td>Llama 4 Scout (10M)</td>
          <td>Gemini 1.5 (2M)</td>
          <td>Open leads</td>
        </tr>
        <tr>
          <td>Multilingual</td>
          <td>DeepSeek V3</td>
          <td>GPT-5.1</td>
          <td>~Equal</td>
        </tr>
        <tr>
          <td>Multimodal</td>
          <td>Llama 4</td>
          <td>Gemini 3</td>
          <td>~10%</td>
        </tr>
      </tbody>
    </table>

    <p>
      By late 2025, the gap between open and closed models had effectively closed on most
      benchmarks. In some areas‚Äîparticularly context length‚Äîopen models actually led.
      The narrative shifted from "can open source compete?" to "why pay for closed APIs?"
    </p>

    <h2>The Global Open Source Ecosystem</h2>

    <p>
      Open-source AI became a global movement. Beyond Meta and DeepSeek, key players emerged:
    </p>

    <div class="milestone">
      <div class="milestone-header">
        <span class="milestone-date">Throughout 2025</span>
        <span class="milestone-significance">‚≠ê</span>
      </div>
      <h3 class="milestone-title">Global Open Source Labs</h3>
      <p class="milestone-desc">
        <strong>Mistral AI (France)</strong> continued releasing efficient models that punched above
        their weight. <strong>Alibaba's Qwen</strong> models dominated Chinese-language tasks.
        <strong>G42's Falcon</strong> (UAE) targeted Arabic markets. <strong>AI2's OLMo</strong>
        provided fully transparent training for researchers.
      </p>
      <div class="so-what">
        <div class="so-what-label">So What?</div>
        <p class="so-what-text">
          <strong>For practitioners:</strong> Open source is no longer a single company's effort.
          Multiple competitive options exist for different use cases, languages, and regions.
          The ecosystem is robust and self-sustaining.
        </p>
      </div>
    </div>

    <h2>The Full Timeline</h2>

    <div class="timeline">
      <div class="timeline-item">
        <div class="timeline-date">February 2023</div>
        <div class="timeline-title">Llama 1 Leaked</div>
        <div class="timeline-desc">Meta's original Llama leaked online, sparking open-source interest</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">July 2023</div>
        <div class="timeline-title">Llama 2 Released</div>
        <div class="timeline-desc">First officially open commercial-use LLM at scale</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">September 2023</div>
        <div class="timeline-title">Mistral 7B</div>
        <div class="timeline-desc">French startup releases surprisingly capable small model</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">December 2023</div>
        <div class="timeline-title">Mixtral 8x7B</div>
        <div class="timeline-desc">First open MoE model demonstrates efficiency gains</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">April 2024</div>
        <div class="timeline-title">Llama 3</div>
        <div class="timeline-desc">Meta closes gap with GPT-4 class performance</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">November 2024</div>
        <div class="timeline-title">DeepSeek V2</div>
        <div class="timeline-desc">Chinese lab demonstrates efficient training methods</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">January 2025</div>
        <div class="timeline-title">DeepSeek R1</div>
        <div class="timeline-desc">Open-source reasoning matches o1, $500B Nvidia drop</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">April 2025</div>
        <div class="timeline-title">Llama 4 Family</div>
        <div class="timeline-desc">First open MoE + multimodal + 10M context model</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">April 2025</div>
        <div class="timeline-title">G42 Falcon 3</div>
        <div class="timeline-desc">UAE releases Arabic-optimized open models</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">July 2025</div>
        <div class="timeline-title">AI2 OLMo 2</div>
        <div class="timeline-desc">Fully transparent training for research community</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">October 2025</div>
        <div class="timeline-title">Llama Federal Approval</div>
        <div class="timeline-desc">US government approves Llama for federal use</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">November 2025</div>
        <div class="timeline-title">DeepSeek V3 Preview</div>
        <div class="timeline-desc">China continues pushing open-source frontier</div>
      </div>
      <div class="timeline-item">
        <div class="timeline-date">December 2025</div>
        <div class="timeline-title">DeepSeek IMO Gold</div>
        <div class="timeline-desc">Open-source model achieves math olympiad gold</div>
      </div>
    </div>

    <h2>Key Takeaways for Practitioners</h2>

    <div class="takeaways">
      <h3 class="takeaways-title">What This Means For You</h3>
      <ul class="takeaways-list">
        <li>
          <strong>Open source is production-ready.</strong> Llama 4 and DeepSeek R1 are
          competitive with the best closed models. You can build products without API
          dependencies.
        </li>
        <li>
          <strong>Self-hosting is viable.</strong> Quantization and MoE architectures mean
          you can run capable models on reasonable hardware. The economics favor ownership
          for sustained workloads.
        </li>
        <li>
          <strong>Fine-tuning is your moat.</strong> Open weights let you customize models
          for your domain. This creates differentiation that API access can't provide.
        </li>
        <li>
          <strong>The ecosystem is mature.</strong> Hugging Face, vLLM, and countless tools
          make open-source deployment straightforward. You're not pioneering anymore.
        </li>
        <li>
          <strong>Watch DeepSeek and Qwen.</strong> Chinese labs are shipping frontier-class
          open models with innovative efficiency techniques. Don't ignore them because of
          geography.
        </li>
      </ul>
    </div>

    <div class="next-path">
      <div class="next-path-label">Continue Learning</div>
      <h3 class="next-path-title">Rise of AI Agents</h3>
      <p style="color: var(--text-secondary); margin-bottom: var(--space-lg);">
        From chatbots to autonomous workers. How Claude Computer Use, ChatGPT Agent,
        and MCP are enabling AI to take action.
      </p>
      <a href="ai-agents.html" class="next-path-link">
        Start Next Path
        <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
          <path d="M5 12h14M12 5l7 7-7 7"/>
        </svg>
      </a>
    </div>
  </article>

  <footer class="footer">
    <p class="footer-text">
      <a href="index.html">All Learning Paths</a> ¬∑
      <a href="../timeline.html">Visual Timeline</a> ¬∑
      <a href="../docs.html">Documentation</a>
    </p>
  </footer>

  <script>
    const progressBar = document.getElementById('progressBar');
    window.addEventListener('scroll', () => {
      const scrollTop = window.scrollY;
      const docHeight = document.documentElement.scrollHeight - window.innerHeight;
      const progress = (scrollTop / docHeight) * 100;
      progressBar.style.width = progress + '%';
    });

    const nav = document.getElementById('nav');
    window.addEventListener('scroll', () => {
      nav.classList.toggle('scrolled', window.scrollY > 50);
    });

    const themeToggle = document.getElementById('themeToggle');
    function getPreferredTheme() {
      const saved = localStorage.getItem('theme');
      if (saved) return saved;
      return window.matchMedia('(prefers-color-scheme: light)').matches ? 'light' : 'dark';
    }
    function setTheme(theme) {
      document.documentElement.setAttribute('data-theme', theme);
      localStorage.setItem('theme', theme);
      const icon = themeToggle.querySelector('svg');
      if (theme === 'light') {
        icon.innerHTML = `<path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/>`;
      } else {
        icon.innerHTML = `<circle cx="12" cy="12" r="5"/><path d="M12 1v2M12 21v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M1 12h2M21 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42"/>`;
      }
    }
    themeToggle.addEventListener('click', () => {
      const current = document.documentElement.getAttribute('data-theme');
      setTheme(current === 'dark' ? 'light' : 'dark');
    });
    setTheme(getPreferredTheme());
  </script>
</body>
</html>
